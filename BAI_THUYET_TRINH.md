# THUYáº¾T TRÃŒNH Dá»° ÃN: Há»† THá»NG Táº O CHÃš THÃCH áº¢NH Tá»° Äá»˜NG

---

## PHáº¦N 1: GIá»šI THIá»†U

### 1.1. BÃ i toÃ¡n cáº§n giáº£i quyáº¿t
Trong thá»i Ä‘áº¡i sá»‘ hÃ³a, hÃ ng triá»‡u bá»©c áº£nh Ä‘Æ°á»£c táº£i lÃªn Internet má»—i ngÃ y. Viá»‡c tá»± Ä‘á»™ng táº¡o mÃ´ táº£ (caption) cho áº£nh cÃ³ Ã½ nghÄ©a quan trá»ng:
- **Há»— trá»£ ngÆ°á»i khiáº¿m thá»‹:** Äá»c mÃ´ táº£ áº£nh qua cÃ´ng nghá»‡ text-to-speech
- **TÃ¬m kiáº¿m áº£nh:** TÃ¬m kiáº¿m báº±ng ngÃ´n ngá»¯ tá»± nhiÃªn thay vÃ¬ tags thá»§ cÃ´ng
- **Quáº£n lÃ½ áº£nh:** Tá»± Ä‘á»™ng phÃ¢n loáº¡i vÃ  gáº¯n nhÃ£n áº£nh
- **Máº¡ng xÃ£ há»™i:** Gá»£i Ã½ caption cho ngÆ°á»i dÃ¹ng

### 1.2. ThÃ¡ch thá»©c
- Hiá»ƒu Ä‘Æ°á»£c **ná»™i dung áº£nh** (cÃ³ gÃ¬ trong áº£nh?)
- PhÃ¢n tÃ­ch **má»‘i quan há»‡** giá»¯a cÃ¡c Ä‘á»‘i tÆ°á»£ng
- Sinh **cÃ¢u tá»± nhiÃªn** mÃ´ táº£ chÃ­nh xÃ¡c

### 1.3. Giáº£i phÃ¡p Ä‘á» xuáº¥t
XÃ¢y dá»±ng há»‡ thá»‘ng **end-to-end** káº¿t há»£p 3 cÃ´ng nghá»‡ AI tiÃªn tiáº¿n:
1. **Faster R-CNN** - PhÃ¡t hiá»‡n Ä‘á»‘i tÆ°á»£ng
2. **Relationship Graph** - PhÃ¢n tÃ­ch má»‘i quan há»‡
3. **LSTM** - Sinh ngÃ´n ngá»¯ tá»± nhiÃªn

---

## PHáº¦N 2: Dá»® LIá»†U NGHIÃŠN Cá»¨U

### 2.1. Dataset: Flickr8k
**Flickr8k** lÃ  bá»™ dataset chuáº©n trong nghiÃªn cá»©u Image Captioning:
- **Nguá»“n:** áº¢nh tá»« Flickr.com
- **Quy mÃ´:** 8,091 áº£nh
- **Ná»™i dung:** áº¢nh vá» Ä‘á»i sá»‘ng hÃ ng ngÃ y (ngÆ°á»i, Ä‘á»™ng váº­t, phong cáº£nh, hoáº¡t Ä‘á»™ng...)
- **Annotation:** Má»—i áº£nh cÃ³ caption mÃ´ táº£ Ä‘Æ°á»£c viáº¿t bá»Ÿi con ngÆ°á»i

### 2.2. Chuáº©n bá»‹ dá»¯ liá»‡u
**Chia dataset theo tá»· lá»‡ 80/20:**
- **Training set:** 6,472 áº£nh (80%) - DÃ¹ng Ä‘á»ƒ huáº¥n luyá»‡n model
- **Test set:** 1,619 áº£nh (20%) - DÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c

**Äáº£m báº£o tÃ­nh khoa há»c:**
- Shuffle ngáº«u nhiÃªn vá»›i seed cá»‘ Ä‘á»‹nh (seed=42)
- LÆ°u danh sÃ¡ch split vÃ o `data_splits.json` Ä‘á»ƒ cÃ³ thá»ƒ tÃ¡i táº¡o káº¿t quáº£
- Test set **hoÃ n toÃ n tÃ¡ch biá»‡t**, model khÃ´ng Ä‘Æ°á»£c "nhÃ¬n tháº¥y" trong quÃ¡ trÃ¬nh training

**Dá»¯ liá»‡u bá»• sung:**
- `boxes_rel.json`: Bounding boxes cho cÃ¡c objects trong áº£nh (tá»± táº¡o báº±ng pretrained detector)
- `captions.txt`: Ground truth captions cho training

---

## PHáº¦N 3: KIáº¾N TRÃšC Há»† THá»NG

### 3.1. Tá»•ng quan Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   INPUT:    â”‚
â”‚   áº¢nh RGB   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MODULE 1: Faster R-CNN      â”‚
â”‚  â†’ PhÃ¡t hiá»‡n Ä‘á»‘i tÆ°á»£ng       â”‚
â”‚  â†’ TrÃ­ch xuáº¥t features       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Object features
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MODULE 2: Relationship GNN  â”‚
â”‚  â†’ XÃ¢y dá»±ng scene graph      â”‚
â”‚  â†’ PhÃ¢n tÃ­ch má»‘i quan há»‡     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Enhanced features
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MODULE 3: LSTM Generator    â”‚
â”‚  â†’ Attention mechanism       â”‚
â”‚  â†’ Sinh caption tá»«ng tá»«      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   OUTPUT:   â”‚
â”‚   Caption   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2. Module 1: Faster R-CNN (Object Detection)

**Vai trÃ²:** "Con máº¯t" cá»§a há»‡ thá»‘ng - nhÃ¬n vÃ  nháº­n diá»‡n Ä‘á»‘i tÆ°á»£ng trong áº£nh

**Kiáº¿n trÃºc:**
- **Backbone:** VGG16 pretrained trÃªn ImageNet
  - ÄÃ£ há»c Ä‘Æ°á»£c cÃ¡c Ä‘áº·c trÆ°ng cÆ¡ báº£n (cáº¡nh, gÃ³c, texture, patterns...)
  - Extract feature maps tá»« áº£nh Ä‘áº§u vÃ o
  
- **Region Proposal Network (RPN):**
  - Äá» xuáº¥t cÃ¡c vÃ¹ng cÃ³ kháº£ nÄƒng chá»©a objects
  - Loáº¡i bá» background khÃ´ng cáº§n thiáº¿t
  
- **ROI Pooling & Classification:**
  - Resize cÃ¡c vÃ¹ng Ä‘á» xuáº¥t vá» kÃ­ch thÆ°á»›c cá»‘ Ä‘á»‹nh
  - PhÃ¢n loáº¡i object vÃ  tinh chá»‰nh bounding box

**Output:**
- **Bounding boxes:** Tá»a Ä‘á»™ (x1, y1, x2, y2) cá»§a má»—i object
- **Labels:** NhÃ£n phÃ¢n loáº¡i (person, dog, car, tree...)
- **Feature vectors:** Vector Ä‘áº·c trÆ°ng 512-dim cho má»—i object

**VÃ­ dá»¥:** áº¢nh cÃ³ 2 ngÆ°á»i Ä‘ang chÆ¡i vá»›i chÃ³ â†’ Detect 3 objects: person #1, person #2, dog

### 3.3. Module 2: Relationship Graph (Graph Neural Network)

**Vai trÃ²:** "Bá»™ nÃ£o phÃ¢n tÃ­ch" - hiá»ƒu má»‘i quan há»‡ khÃ´ng gian vÃ  ngá»¯ nghÄ©a giá»¯a cÃ¡c objects

**Táº¡i sao cáº§n Relationship Graph?**
- Faster R-CNN chá»‰ detect objects **riÃªng láº»**
- NhÆ°ng caption cáº§n hiá»ƒu **ngá»¯ cáº£nh**: "ngÆ°á»i Ä‘ang nÃ©m bÃ³ng cho chÃ³" (khÃ´ng chá»‰ "ngÆ°á»i + bÃ³ng + chÃ³")

**CÃ¡ch hoáº¡t Ä‘á»™ng:**
1. **XÃ¢y dá»±ng Scene Graph:**
   - Má»—i object = 1 node
   - Káº¿t ná»‘i cÃ¡c nodes thÃ nh graph dá»±a trÃªn vá»‹ trÃ­ khÃ´ng gian
   
2. **Graph Neural Network:**
   - Message passing giá»¯a cÃ¡c nodes
   - Má»—i node "há»c" thÃ´ng tin tá»« neighbors
   - Cáº­p nháº­t features dá»±a trÃªn ngá»¯ cáº£nh xung quanh

3. **Relationship Classification:**
   - PhÃ¢n loáº¡i má»‘i quan há»‡: above, next to, holding, riding, wearing...
   - TÃ­ch há»£p relationship features vÃ o object features

**Output:**
- **Enhanced object features:** Vector Ä‘áº·c trÆ°ng Ä‘Ã£ bá»• sung thÃ´ng tin vá» má»‘i quan há»‡
- **Relationship matrix:** Ma tráº­n thá»ƒ hiá»‡n má»‘i quan há»‡ giá»¯a cÃ¡c cáº·p objects

**VÃ­ dá»¥:** 
- Detect: person, ball, dog
- Graph: person -[throwing]â†’ ball -[to]â†’ dog
- Features giá» chá»©a thÃ´ng tin vá» action "throwing"

### 3.4. Module 3: LSTM Caption Generator

**Vai trÃ²:** "NgÆ°á»i ká»ƒ chuyá»‡n" - chuyá»ƒn visual features thÃ nh ngÃ´n ngá»¯ tá»± nhiÃªn

**Kiáº¿n trÃºc:**

1. **Vocabulary (Tá»« Ä‘iá»ƒn):**
   - XÃ¢y dá»±ng tá»« 8,091 captions trong dataset
   - KÃ­ch thÆ°á»›c: 5,320 tá»« duy nháº¥t
   - Special tokens: `<start>`, `<end>`, `<pad>`, `<unk>`

2. **LSTM (Long Short-Term Memory):**
   - **2 layers LSTM** vá»›i hidden size 512
   - Nhá»› Ä‘Æ°á»£c ngá»¯ cáº£nh dÃ i (long-term dependencies)
   - Sinh caption **tuáº§n tá»± tá»«ng tá»« má»™t**

3. **Attention Mechanism:**
   - Táº¡i má»—i time step, LSTM "chÃº Ã½" vÃ o cÃ¡c objects khÃ¡c nhau
   - VÃ­ dá»¥: Khi sinh tá»« "throwing", attention táº­p trung vÃ o person; khi sinh "ball", attention táº­p trung vÃ o ball
   - GiÃºp model linh hoáº¡t vÃ  chÃ­nh xÃ¡c hÆ¡n

**QuÃ¡ trÃ¬nh sinh caption:**
```
Step 1: [<start>] â†’ "a"
Step 2: [<start>, a] â†’ "person"
Step 3: [<start>, a, person] â†’ "is"
Step 4: [<start>, a, person, is] â†’ "throwing"
Step 5: [<start>, a, person, is, throwing] â†’ "a"
Step 6: [<start>, a, person, is, throwing, a] â†’ "ball"
...
Step N: [...] â†’ <end>
```

**Output:** CÃ¢u caption hoÃ n chá»‰nh: "a person is throwing a ball to a dog"

---

## PHáº¦N 4: PHÆ¯Æ NG PHÃP VÃ€ THUáº¬T TOÃN

### 4.1. Tá»•ng quan phÆ°Æ¡ng phÃ¡p

**BÃ i toÃ¡n:** Cho áº£nh Ä‘áº§u vÃ o I, sinh cÃ¢u mÃ´ táº£ (caption) C = {wâ‚, wâ‚‚, ..., wâ‚™}

**Giáº£i phÃ¡p:** Káº¿t há»£p 3 bÆ°á»›c xá»­ lÃ½ tuáº§n tá»± trong má»™t pipeline end-to-end

```
I (áº£nh) â†’ [Faster R-CNN] â†’ Features â†’ [Graph] â†’ Enhanced Features â†’ [LSTM] â†’ C (caption)
```

### 4.2. Thuáº­t toÃ¡n Faster R-CNN

**Input:** áº¢nh RGB kÃ­ch thÆ°á»›c HÃ—WÃ—3

**BÆ°á»›c 1: Feature Extraction (VGG16 Backbone)**
```
1. Resize áº£nh vá» max_size = 500px (giá»¯ nguyÃªn tá»· lá»‡)
2. Normalize: pixel = (pixel - mean) / std
3. Forward qua VGG16:
   - Conv layers: 13 lá»›p convolution
   - Output: Feature map kÃ­ch thÆ°á»›c H/16 Ã— W/16 Ã— 512
```

**BÆ°á»›c 2: Region Proposal Network (RPN)**
```
Input: Feature map (H/16 Ã— W/16 Ã— 512)

For each position (i, j) trÃªn feature map:
    1. Táº¡o 9 anchor boxes vá»›i tá»· lá»‡ khÃ¡c nhau:
       - 3 scales: [128Â², 256Â², 512Â²]
       - 3 ratios: [0.5, 1, 2]
    
    2. PhÃ¢n loáº¡i objectness:
       - P(object) = sigmoid(score)
       - Náº¿u P(object) > 0.5 â†’ giá»¯ láº¡i
    
    3. Regression Ä‘á»ƒ tinh chá»‰nh box:
       - Î”x, Î”y, Î”w, Î”h = RPN_regressor(features)
       - box_refined = anchor + (Î”x, Î”y, Î”w, Î”h)

Output: ~2000 region proposals
```

**BÆ°á»›c 3: ROI Pooling & Classification**
```
For each region proposal:
    1. ROI Pooling:
       - Crop feature tÆ°Æ¡ng á»©ng tá»« feature map
       - Resize vá» kÃ­ch thÆ°á»›c cá»‘ Ä‘á»‹nh 7Ã—7
    
    2. Flatten vÃ  qua Fully Connected layers:
       - FC1: 7Ã—7Ã—512 â†’ 4096
       - FC2: 4096 â†’ 512 (object features)
    
    3. Classification:
       - Softmax: 512 â†’ 10 classes
       - class_id = argmax(scores)
    
    4. Box Regression (tinh chá»‰nh láº§n 2):
       - Adjust bounding box coordinates

Output: 
- Bounding boxes: [(x1, y1, x2, y2), ...]
- Labels: [class_id, ...]
- Features: [512-dim vector, ...] cho má»—i object
```

### 4.3. Thuáº­t toÃ¡n Relationship Graph

**Input:** 
- Object features: F = [fâ‚, fâ‚‚, ..., fâ‚™] (n objects, má»—i cÃ¡i 512-dim)
- Bounding boxes: B = [(xâ‚, yâ‚, xâ‚‚, yâ‚‚), ...]

**BÆ°á»›c 1: XÃ¢y dá»±ng Scene Graph**
```
Khá»Ÿi táº¡o graph G = (V, E):
- Nodes V = {vâ‚, vâ‚‚, ..., vâ‚™} (má»—i object lÃ  1 node)
- Edges E = {}

For i from 1 to n:
    For j from 1 to n (j â‰  i):
        1. TÃ­nh spatial features:
           - distance = ||center(box_i) - center(box_j)||
           - relative_position = (x_j - x_i, y_j - y_i) / image_size
           - IoU = intersection(box_i, box_j) / union(box_i, box_j)
        
        2. Káº¿t ná»‘i node:
           - If distance < threshold:
               E = E âˆª {edge(i â†’ j)}
               edge_features[i,j] = concat(f_i, f_j, spatial_features)
```

**BÆ°á»›c 2: Graph Neural Network (Message Passing)**
```
For each layer l in [1, 2, 3]:  # 3 layers GNN
    For each node v_i:
        1. Thu tháº­p messages tá»« neighbors:
           messages = []
           For each neighbor v_j:
               m_ij = MLP(concat(h_i^(l-1), h_j^(l-1), edge_features[i,j]))
               messages.append(m_ij)
        
        2. Aggregate messages:
           aggregated = mean(messages)  # hoáº·c max, sum
        
        3. Update node features:
           h_i^(l) = ReLU(W^(l) Ã— concat(h_i^(l-1), aggregated) + b^(l))

Output: Enhanced features h_i^(3) cho má»—i object
```

**BÆ°á»›c 3: Relationship Classification**
```
For each pair (i, j):
    1. Concat features:
       pair_feature = concat(h_i^(3), h_j^(3), spatial_features[i,j])
    
    2. PhÃ¢n loáº¡i relation:
       rel_logits = FC_relation(pair_feature)  # 6 classes
       rel_type = argmax(rel_logits)
       # Classes: above, below, left, right, holding, wearing

Output: Relationship matrix R (nÃ—n)
```

### 4.4. Thuáº­t toÃ¡n LSTM Caption Generator

**Input:**
- Enhanced object features: H = [hâ‚, hâ‚‚, ..., hâ‚™]
- Ground truth caption (khi training): C = [wâ‚, wâ‚‚, ..., wâ‚˜]

**BÆ°á»›c 1: Khá»Ÿi táº¡o**
```
1. Vocabulary V: 5,320 tá»« (xÃ¢y tá»« dataset)
   - word2idx: {"a": 1, "dog": 2, "is": 3, ...}
   - idx2word: {1: "a", 2: "dog", 3: "is", ...}
   - Special tokens: <start>=0, <end>=5319, <pad>=5318

2. TÃ­nh global image feature:
   v_global = mean(H)  # Average pooling trÃªn táº¥t cáº£ objects
   
3. Khá»Ÿi táº¡o LSTM state:
   hâ‚€ = tanh(W_init Ã— v_global)
   câ‚€ = zeros(512)
```

**BÆ°á»›c 2: Attention Mechanism**
```
Function Attention(h_t, H):
    """TÃ­nh attention weights táº¡i time step t"""
    
    For each object feature h_i in H:
        # TÃ­nh attention score
        score_i = (W_h Ã— h_t)áµ€ Ã— (W_v Ã— h_i)
    
    # Normalize báº±ng softmax
    attention_weights = softmax([score_1, score_2, ..., score_n])
    
    # Weighted sum
    context_vector = Î£(attention_weights[i] Ã— h_i)
    
    Return context_vector, attention_weights
```

**BÆ°á»›c 3: Sinh Caption (Training Mode)**
```
Input: Ground truth caption C = [<start>, wâ‚, wâ‚‚, ..., wâ‚˜, <end>]

h_t = hâ‚€, c_t = câ‚€
total_loss = 0

For t from 1 to m+1:
    1. Embedding tá»« hiá»‡n táº¡i:
       x_t = Embedding[C[t-1]]  # 512-dim word embedding
    
    2. Attention:
       context_t, Î±_t = Attention(h_t, H)
    
    3. Concat input:
       input_t = concat(x_t, context_t)
    
    4. LSTM forward:
       h_t, c_t = LSTM(input_t, h_{t-1}, c_{t-1})
    
    5. Predict next word:
       logits_t = FC(h_t)  # 512 â†’ 5320 (vocab size)
       probs_t = softmax(logits_t)
    
    6. TÃ­nh loss:
       loss_t = -log(probs_t[C[t]])  # Cross-entropy
       total_loss += loss_t

Return: total_loss / (m+1)
```

**BÆ°á»›c 4: Sinh Caption (Inference Mode)**
```
h_t = hâ‚€, c_t = câ‚€
caption = [<start>]

For t from 1 to max_length (=20):
    1. Embedding tá»« vá»«a sinh:
       x_t = Embedding[caption[-1]]
    
    2. Attention:
       context_t, Î±_t = Attention(h_t, H)
    
    3. LSTM forward:
       input_t = concat(x_t, context_t)
       h_t, c_t = LSTM(input_t, h_{t-1}, c_{t-1})
    
    4. Greedy decoding:
       logits_t = FC(h_t)
       word_id = argmax(softmax(logits_t))
    
    5. ThÃªm vÃ o caption:
       caption.append(word_id)
    
    6. Dá»«ng náº¿u gáº·p <end>:
       If word_id == <end>:
           Break

Return: caption chuyá»ƒn tá»« IDs sang words
```

### 4.5. Thuáº­t toÃ¡n Training End-to-End

**Pseudo-code tá»•ng thá»ƒ:**

```python
# ===== KHá»I Táº O =====
model = CombinedModel(FasterRCNN, RelationshipGraph, LSTM)
optimizer = Adam(lr=5e-5)
scheduler = CosineAnnealingLR(T_max=12)

train_data = load_split("train")  # 6,472 images
test_data = load_split("test")    # 1,619 images

best_loss = âˆ

# ===== TRAINING LOOP =====
For epoch in range(1, 13):
    
    # --- TRAINING PHASE ---
    model.train()
    train_losses = []
    
    For each (image, caption, boxes) in train_data:
        1. Forward pass:
           # Faster R-CNN
           obj_features, boxes, labels = FasterRCNN(image)
           
           # Relationship Graph
           enhanced_features = RelationshipGraph(obj_features, boxes)
           
           # LSTM
           caption_loss = LSTM.forward(enhanced_features, caption)
        
        2. Backward pass:
           optimizer.zero_grad()
           caption_loss.backward()  # Backprop qua toÃ n bá»™ pipeline
           clip_grad_norm(model.parameters(), max_norm=1.0)
           optimizer.step()
        
        3. Log loss:
           train_losses.append(caption_loss.item())
    
    avg_train_loss = mean(train_losses)
    
    # --- TESTING PHASE ---
    model.eval()
    test_losses = []
    
    For each (image, caption, boxes) in test_data:
        With no_grad():
            obj_features = FasterRCNN(image)
            enhanced_features = RelationshipGraph(obj_features, boxes)
            caption_loss = LSTM.forward(enhanced_features, caption)
            test_losses.append(caption_loss.item())
    
    avg_test_loss = mean(test_losses)
    
    # --- CHECKPOINT ---
    save_checkpoint(f"epoch_{epoch}.pth")
    
    If avg_test_loss < best_loss:
        best_loss = avg_test_loss
        save_checkpoint("best_12ep.pth")
        patience_counter = 0
    Else:
        patience_counter += 1
    
    # Early stopping
    If patience_counter >= 5:
        Break
    
    # Update learning rate
    scheduler.step()
```

### 4.6. Äá»™ phá»©c táº¡p tÃ­nh toÃ¡n

**Faster R-CNN:**
- Backbone VGG16: O(HÃ—WÃ—C) vá»›i C=512 channels
- RPN: O(H/16 Ã— W/16 Ã— 9) ~ O(HW)
- ROI Pooling: O(RÃ—7Ã—7) vá»›i R=sá»‘ regions (~2000)
- **Tá»•ng:** O(HW + R)

**Relationship Graph:**
- Build graph: O(nÂ²) vá»›i n=sá»‘ objects
- GNN layers: O(LÃ—nÂ²Ã—d) vá»›i L=3 layers, d=512 features
- **Tá»•ng:** O(nÂ²Ã—d)

**LSTM:**
- Má»—i time step: O(dÂ²) vá»›i d=512
- T time steps: O(TÃ—dÂ²) vá»›i T~15-20 tá»«
- Attention: O(TÃ—nÃ—d)
- **Tá»•ng:** O(TÃ—(dÂ²+nÃ—d))

**Training:**
- 1 epoch: 6,472 áº£nh Ã— (O(HW) + O(nÂ²d) + O(TdÂ²))
- 12 epochs Ã— 6,472 = 77,664 forward+backward passes

---

## PHáº¦N 5: QUÃ TRÃŒNH HUáº¤N LUYá»†N

### 5.1. Chuáº©n bá»‹ dá»¯ liá»‡u

**Äiá»ƒm Ä‘áº·c biá»‡t:** Cáº£ 3 modules Ä‘Æ°á»£c train **Ä‘á»“ng thá»i** (end-to-end), khÃ´ng train riÃªng láº»

**Lá»£i Ã­ch:**
- CÃ¡c module há»c cÃ¡ch **phá»‘i há»£p** vá»›i nhau
- Faster R-CNN há»c extract features **tá»‘t cho captioning**, khÃ´ng chá»‰ cho detection
- Relationship Graph há»c nhá»¯ng quan há»‡ **quan trá»ng cho mÃ´ táº£**
- LSTM há»c cÃ¡ch **táº­n dá»¥ng tá»‘i Ä‘a** visual features

### 5.1. Chuáº©n bá»‹ dá»¯ liá»‡u

**Quy trÃ¬nh xá»­ lÃ½ dataset:**

```python
# 1. Load vÃ  split dataset
images = load_images("Images/")  # 8,091 áº£nh
captions = load_captions("captions.txt")
boxes = load_boxes("boxes_rel.json")

# 2. Shuffle vá»›i seed
random.seed(42)
indices = list(range(8091))
random.shuffle(indices)

# 3. Split 80/20
split_point = int(0.8 Ã— 8091) = 6472
train_indices = indices[:6472]
test_indices = indices[6472:]

# 4. LÆ°u splits
save_json({"train": train_indices, "test": test_indices}, "data_splits.json")
```

**Dataset class:**
```python
class Flickr8kDataset:
    def __getitem__(self, idx):
        # Load áº£nh
        image = Image.open(image_path).convert('RGB')
        image = ToTensor()(image)  # Normalize [0,1]
        
        # Load boxes & labels
        boxes = boxes_data[image_id]['boxes']
        labels = boxes_data[image_id]['labels']
        
        # Load caption vÃ  encode
        caption_text = captions[image_id]
        caption_ids = vocabulary.encode(caption_text)
        
        return {
            'image': image,
            'boxes': tensor(boxes),
            'labels': tensor(labels),
            'caption': tensor(caption_ids)
        }
```

### 5.2. Training End-to-End

**Hardware:**
- GPU: NVIDIA CUDA-enabled
- RAM: 16GB+
- Storage: ~3-4GB cho checkpoints

**Hyperparameters:**
- **Sá»‘ epochs:** 12
- **Optimizer:** Adam
- **Learning rate:** 5e-5 (0.00005)
- **Learning rate schedule:** Cosine Annealing (giáº£m dáº§n theo dáº¡ng cos)
- **Weight decay:** 1e-5 (regularization)
- **Batch size:** 1 (do áº£nh cÃ³ kÃ­ch thÆ°á»›c khÃ¡c nhau)

### 5.2. Training End-to-End

**Äiá»ƒm Ä‘áº·c biá»‡t:** Cáº£ 3 modules Ä‘Æ°á»£c train **Ä‘á»“ng thá»i** (end-to-end), khÃ´ng train riÃªng láº»

**Lá»£i Ã­ch:**
- CÃ¡c module há»c cÃ¡ch **phá»‘i há»£p** vá»›i nhau
- Faster R-CNN há»c extract features **tá»‘t cho captioning**, khÃ´ng chá»‰ cho detection
- Relationship Graph há»c nhá»¯ng quan há»‡ **quan trá»ng cho mÃ´ táº£**
- LSTM há»c cÃ¡ch **táº­n dá»¥ng tá»‘i Ä‘a** visual features

### 5.3. Cáº¥u hÃ¬nh Training

**TRAINING PHASE (6,472 áº£nh):**

```python
For each image in training set:
    1. Load áº£nh + caption ground truth
    2. Forward pass:
       - Faster R-CNN â†’ detect objects
       - Relationship Graph â†’ analyze relationships
       - LSTM â†’ generate caption
    3. TÃ­nh loss:
       - So sÃ¡nh caption sinh ra vá»›i ground truth
       - Cross-entropy loss cho má»—i tá»«
    4. Backward pass:
       - Backpropagation qua toÃ n bá»™ pipeline
       - Update weights cá»§a cáº£ 3 modules
```

**TESTING PHASE (1,619 áº£nh):**

```python
For each image in test set:
    1. Load áº£nh + caption ground truth
    2. Forward pass (no gradient):
       - Generate caption nhÆ° training
    3. TÃ­nh test loss:
       - ÄÃ¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c
       - KHÃ”NG update weights
```

### 5.4. Quy trÃ¬nh má»—i Epoch

**Caption Loss (Cross-Entropy):**
```
Má»—i tá»« Ä‘Æ°á»£c predict cÃ³ má»™t xÃ¡c suáº¥t phÃ¢n phá»‘i trÃªn 5,320 tá»«
Loss = -log(P(tá»« Ä‘Ãºng))

VÃ­ dá»¥:
Ground truth: "dog"
Model predict: {cat: 0.3, dog: 0.6, bird: 0.1}
Loss = -log(0.6) = 0.51
```

**Total Loss:**
```
loss = caption_loss + 0.1 Ã— relationship_loss
```

### 5.5. Loss Function

**Chiáº¿n lÆ°á»£c lÆ°u model:**

1. **Checkpoint má»—i epoch:**
   - `frcnn_caption_epoch_01.pth`
   - `frcnn_caption_epoch_02.pth`
   - ...
   - `frcnn_caption_epoch_12.pth`
   - **Má»¥c Ä‘Ã­ch:** Resume training náº¿u bá»‹ giÃ¡n Ä‘oáº¡n

2. **Best model:**
   - `frcnn_caption_best_12ep.pth`
   - LÆ°u model cÃ³ **test loss tháº¥p nháº¥t**
   - **Quan trá»ng:** Chá»‰ xÃ©t test loss, khÃ´ng xÃ©t train loss
   - **LÃ½ do:** Test loss thá»ƒ hiá»‡n kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a thá»±c sá»±

**Early Stopping:**
- Náº¿u 5 epochs liÃªn tiáº¿p test loss khÃ´ng giáº£m â†’ dá»«ng training
- TrÃ¡nh lÃ£ng phÃ­ thá»i gian khi model Ä‘Ã£ há»™i tá»¥

### 5.6. Checkpoint & Best Model

**Loss progression:**
```
Epoch 1:  Train 6.05 | Test 6.12  â† Model má»›i báº¯t Ä‘áº§u
Epoch 2:  Train 5.23 | Test 5.41
Epoch 3:  Train 4.68 | Test 4.89
Epoch 4:  Train 4.35 | Test 4.52
Epoch 5:  Train 4.11 | Test 4.28
Epoch 6:  Train 3.94 | Test 4.15  â† Best model
Epoch 7:  Train 4.26 | Test 4.41  â† KhÃ´ng cáº£i thiá»‡n
Epoch 8:  Train 3.85 | Test 4.10  â† Best model má»›i
...
Epoch 12: Train 3.50 | Test 3.82  â† Káº¿t thÃºc
```

**Xu hÆ°á»›ng:**
- Loss giáº£m nhanh á»Ÿ Ä‘áº§u (epoch 1-4)
- Loss giáº£m cháº­m dáº§n vá» cuá»‘i (epoch 8-12)
- Train loss < Test loss lÃ  bÃ¬nh thÆ°á»ng (model fit tá»‘t trÃªn train set)

---

### 5.7. Káº¿t quáº£ Training (dá»± kiáº¿n)

---

## PHáº¦N 6: ÄÃNH GIÃ MÃ” HÃŒNH

**BLEU (Bilingual Evaluation Understudy)** lÃ  metric chuáº©n trong:
- Machine Translation
- Image Captioning
- Text Generation

**Ã tÆ°á»Ÿng:** So sÃ¡nh n-grams giá»¯a caption sinh ra vÃ  ground truth

**4 chá»‰ sá»‘:**

1. **BLEU-1 (Unigram):**
   - Äáº¿m sá»‘ tá»« Ä‘Æ¡n khá»›p
   - ÄÃ¡nh giÃ¡ **tá»« vá»±ng**
   ```
   Generated: "a dog is running"
   Reference: "a brown dog is running fast"
   Matched: "a", "dog", "is", "running" â†’ 4/5 = 80%
   ```

2. **BLEU-2 (Bigram):**
   - Äáº¿m cá»¥m 2 tá»« liÃªn tiáº¿p khá»›p
   - ÄÃ¡nh giÃ¡ **cáº¥u trÃºc ngáº¯n**
   ```
   Generated: "a dog", "dog is", "is running"
   Reference: "a brown", "brown dog", "dog is", "is running", "running fast"
   Matched: "dog is", "is running" â†’ 2/4 = 50%
   ```

3. **BLEU-3 (Trigram):**
   - Äáº¿m cá»¥m 3 tá»« khá»›p
   - ÄÃ¡nh giÃ¡ **cáº¥u trÃºc cÃ¢u**

4. **BLEU-4 (4-gram):**
   - Äáº¿m cá»¥m 4 tá»« khá»›p
   - ÄÃ¡nh giÃ¡ **tÃ­nh tá»± nhiÃªn tá»•ng thá»ƒ**

**Thang Ä‘iá»ƒm:** 0.0 - 1.0 (hoáº·c 0% - 100%)
- BLEU > 0.5: Ráº¥t tá»‘t
- BLEU 0.3-0.5: Tá»‘t
- BLEU 0.2-0.3: Cháº¥p nháº­n Ä‘Æ°á»£c
- BLEU < 0.2: Cáº§n cáº£i thiá»‡n

### 6.1. Metrics: BLEU Score

```python
1. Load best model: frcnn_caption_best_12ep.pth

2. Load test set: 1,619 áº£nh tá»« data_splits.json

3. For each test image:
   - Model sinh caption
   - LÆ°u cáº·p (generated_caption, ground_truth)

4. TÃ­nh BLEU scores:
   - BLEU-1, BLEU-2, BLEU-3, BLEU-4
   - Trung bÃ¬nh trÃªn 1,619 áº£nh test

5. In káº¿t quáº£:
   BLEU-1: 0.XX
   BLEU-2: 0.XX
   BLEU-3: 0.XX
   BLEU-4: 0.XX
```

### 6.2. Quy trÃ¬nh Evaluation

**áº¢nh test:** Má»™t cÃ´ gÃ¡i Ä‘ang ngá»“i trÃªn gháº¿ Ä‘á»c sÃ¡ch

**Ground truth:** "a young woman sitting on a bench reading a book"

**Model generates:** "a girl is sitting on a bench with a book"

**BLEU scores:**
- BLEU-1: 0.75 (7/8 tá»« khá»›p: a, girl/young woman, is/âˆ…, sitting, on, a, bench, with/reading, a, book)
- BLEU-2: 0.55 (cá»¥m 2 tá»« khá»›p: "on a", "a bench")
- BLEU-3: 0.32
- BLEU-4: 0.18

â†’ Caption **cÃ³ nghÄ©a** nhÆ°ng **cáº¥u trÃºc khÃ¡c** má»™t chÃºt

---

### 6.3. VÃ­ dá»¥ Ä‘Ã¡nh giÃ¡

---

## PHáº¦N 7: Káº¾T QUáº¢ VÃ€ ÄÃ“NG GÃ“P

### 7.1. Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c

âœ… **XÃ¢y dá»±ng thÃ nh cÃ´ng pipeline end-to-end** káº¿t há»£p 3 cÃ´ng nghá»‡ AI

âœ… **Training á»•n Ä‘á»‹nh** trÃªn 8,091 áº£nh Flickr8k vá»›i 12 epochs

âœ… **Checkpoint management** hoÃ n chá»‰nh vá»›i best model tracking

âœ… **ÄÃ¡nh giÃ¡ khoa há»c** vá»›i BLEU metrics trÃªn test set riÃªng biá»‡t

âœ… **Reproducible:** Code, data splits, vÃ  checkpoints Ä‘áº§y Ä‘á»§

### 7.2. Äiá»ƒm máº¡nh cá»§a há»‡ thá»‘ng

**1. Kiáº¿n trÃºc toÃ n diá»‡n:**
- KhÃ´ng chá»‰ detect objects mÃ  cÃ²n hiá»ƒu **má»‘i quan há»‡**
- Attention mechanism giÃºp LSTM "nhÃ¬n" Ä‘Ãºng chá»— khi sinh tá»«

**2. Training end-to-end:**
- CÃ¡c module há»c cÃ¡ch phá»‘i há»£p tá»‘i Æ°u
- Gradient flow xuyÃªn suá»‘t pipeline

**3. Dataset split khoa há»c:**
- Test set hoÃ n toÃ n tÃ¡ch biá»‡t
- Äáº£m báº£o Ä‘Ã¡nh giÃ¡ khÃ¡ch quan

**4. CÃ³ thá»ƒ má»Ÿ rá»™ng:**
- Dá»… dÃ ng thay Faster R-CNN báº±ng YOLO, DETR...
- CÃ³ thá»ƒ thay LSTM báº±ng Transformer
- CÃ³ thá»ƒ train trÃªn dataset lá»›n hÆ¡n (MS COCO, Flickr30k)

### 7.3. Háº¡n cháº¿ vÃ  hÆ°á»›ng phÃ¡t triá»ƒn

**Háº¡n cháº¿:**
- Dataset nhá» (8k áº£nh) â†’ giá»›i háº¡n vocabulary vÃ  Ä‘a dáº¡ng
- Batch size = 1 â†’ training cháº­m
- ChÆ°a xá»­ lÃ½ multi-object attention tá»‘t

**HÆ°á»›ng phÃ¡t triá»ƒn:**
- **Dataset lá»›n hÆ¡n:** MS COCO (120k áº£nh), Conceptual Captions (3M áº£nh)
- **Transformer architecture:** Thay LSTM báº±ng Transformer decoder
- **Beam search:** Thay greedy decoding Ä‘á»ƒ sinh caption Ä‘a dáº¡ng hÆ¡n
- **Visual-semantic embedding:** Há»c joint space giá»¯a vision vÃ  language

---

---

## PHáº¦N 8: Káº¾T LUáº¬N

### 8.1. TÃ³m táº¯t

Dá»± Ã¡n Ä‘Ã£ **xÃ¢y dá»±ng thÃ nh cÃ´ng há»‡ thá»‘ng Image Captioning end-to-end** vá»›i:

ğŸ“Š **Dataset:** Flickr8k (8,091 áº£nh, split 80/20)

ğŸ—ï¸ **Kiáº¿n trÃºc:** Faster R-CNN + Relationship GNN + LSTM

âš™ï¸ **Training:** 12 epochs, GPU, checkpoint má»—i epoch

ğŸ“ˆ **ÄÃ¡nh giÃ¡:** BLEU-1/2/3/4 trÃªn test set

### 8.2. Ã nghÄ©a

Há»‡ thá»‘ng nÃ y minh há»a cÃ¡ch:
- **Computer Vision** (nhÃ¬n) káº¿t há»£p **Natural Language Processing** (nÃ³i)
- **Detection** (phÃ¡t hiá»‡n) káº¿t há»£p **Reasoning** (suy luáº­n) káº¿t há»£p **Generation** (sinh ngÃ´n ngá»¯)
- **End-to-end learning** táº¡o ra káº¿t quáº£ tá»‘t hÆ¡n cÃ¡c module riÃªng láº»

### 8.3. á»¨ng dá»¥ng thá»±c táº¿

ğŸ’¡ **Há»— trá»£ ngÆ°á»i khiáº¿m thá»‹** hiá»ƒu ná»™i dung áº£nh

ğŸ” **TÃ¬m kiáº¿m áº£nh** báº±ng ngÃ´n ngá»¯ tá»± nhiÃªn

ğŸ“± **Máº¡ng xÃ£ há»™i** tá»± Ä‘á»™ng gá»£i Ã½ caption

ğŸ¤– **Robotics** giÃºp robot "hiá»ƒu" mÃ´i trÆ°á»ng xung quanh

---

## Cáº¢M Æ N QUÃ THáº¦Y CÃ” ÄÃƒ Láº®NG NGHE! ğŸ™

**CÃ¡c cÃ¢u há»i?** ğŸ’¬
